# unsplash categories
UNSPLASH_CATEGORIES: [ 'wallpapers','3d-renders','travel','nature','street-photography','experimental','textures-patterns',
                       'animals','architecture-interior','fashion-beauty','film','food-drink','people','spirituality',
                       'business-work','athletics','health','arts-culture' ]

BBC_CATEGORIES: ['technology','business','politics','science_and_environment','health','education','entertainment_and_arts']


CLIP_MODEL: ['openai/clip-vit-base-patch32']

MORE_INFO: { 'zero_shot_classification': "CLIP's proficiency in zero-shot classification stems from its training on a vastly
            diverse set of images and corresponding natural language descriptions. Because of this, CLIP is able 
            to identify and categorize objects and concepts, even those it hasn't encountered before. 
            By utilizing this general knowledge and understanding of the relationships between words and images, 
            CLIP can accurately classify new data without the need for explicit training on those specific objects 
            or concepts.",
               'semantic_search': "Semantic search is a search technique that aims to understand the meaning and 
            context of words and phrases in a query rather than just matching keywords. In the context of CLIP, 
            semantic search involves using the model's ability to associate images with natural language descriptions 
            to perform image search using text. This means that a user can enter a natural language query, and CLIP 
            will return images that it believes are relevant to the meaning of the query, even if the images 
            don't contain the exact words in the query. By using semantic search, CLIP enables a more intuitive and 
            natural way of searching for images based on their content.",
               'clustering': "CLIP embeddings can be used to cluster images and text based on their semantic similarities. 
            To do this, each image or piece of text is first passed through CLIP to generate an embedding, which 
             represents the meaning and content of the image or text in a high-dimensional vector space. These embeddings 
             can then be used as input to other algorithms, such as UMAP (Uniform Manifold Approximation and 
             Projection), which reduces the dimensionality of the embeddings to a lower number of dimensions while 
             preserving their underlying structure. This enables the embeddings to be visualized in a way that 
             highlights clusters of similar images or text. By using CLIP embeddings and clustering techniques like UMAP, 
             it becomes possible to group images or text based on their semantic similarities." }

